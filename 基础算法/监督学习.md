监督学习的基本思想是，从给定的数据和标记中构建一个模型，以便对新数据进行预测。这些模型通常是基于数学函数或算法的，主要通过最小化误差来学习。监督学习算法包括线性回归、决策树、支持向量机、神经网络等。

那我就先弄清楚线性回归和决策树两个东西先： 

## 线性回归

## 决策树



### 信息熵的理解

从追寻决策树的原理，中间有对信息熵的应用

什么是信息熵，
信息熵就是平均而言发生一个事件我们得到的信息量大小。所以数学上，信息熵其实是信息量的期望

> 一个事件的信息量就是这个事件发生的概率的负对数
这个是他的数学计算


假设我计算我今天是否出去吃饭的可能性，前面的十天内，我有3天是出去吃的，那么我出去吃的概率是 30%。
熵值越高，数据的不确定性就越高；熵值越低，数据的不确定性就越低。具体的，如果你想计算你每天出去吃饭的信息熵，需要先统计你在一段时间内出去吃饭的次数和总天数，然后使用信息熵公式（H(X) = -∑p_i * log2(p_i)）来计算你每天出去吃饭的信息熵。
对于你的例子，出去吃饭的概率是 30%，即 p = 0.3，所以可以得到：
H(X) = -(0.3 * log2(0.3) + 0.7 * log2(0.7)) = 0.881。

我觉得log2 是类似于一种二元性的思考，是与否的一个裁定。

当我在是否当中的裁定到的概率一般是0.3的概率了，那么在这个二元区分中，作为了叠加的效果
要得到这个2的几次方，将这个二元选择中细分为一个整体，所以取负数，作为1/n，这个n需要在哪里
才会使他的前后整体概率在了0.3的位置。对于10天内有3天出去吃饭的信息中，且后续再乘以这个概率

当我设定为5天的时候， log2(0.5)就会等以-1， 对于这种是与否都比较均匀的情况中，没有出现从两头递增或递减的曲线，我觉得这个log2在二元选择中属于一种“偏好”的运算。如果是凹下去的曲线函数，大概是一种“厌恶”的运算。
越到结果越偏好，回归值越贴近。
当然，而在不发生到发生存在很微量的确定值或者概率的话，负值就趋向很大，那么这个时候对于这个看起来很不可能发生的事情又增多了他的未知性，万一他发生了，就会吓死人的程度更大，因为他本来发生的概念很小。所以这就是信息熵的投射到现实中人逻辑感知的理解。

科比来了，他走进了电影院：

情形1： 如果电影院有1排座位，他坐在了第1排，此时，我们无法推断：他是否喜欢坐在最前面
可见，如果一个事件A：坐在第1排，是确定的，则对我们推断他的喜好没有帮助，不能减少不确定性，因此：不确定性和结果的可能性有关，A事件的信息量是- log(1)=0

情形2： 如果电影院有2排座位，他坐在了第1排，如果随机落座，坐在第一排的可能性是50%
显然，坐在第1排既然发生了，那么，相比他走进电影院之前，我们对于推断他喜欢坐在最前面，增加了一些把握，减少了不确定性，A事件的信息量是： - log(0.5)

情形3： 如果电影院有100排座位，他坐在第1排，如果随机落座，坐在第一排的可能性只有1%，
那么，显然，此时相比情形2，我们更能推断他喜欢第1排的座位，该事件的信息量是： - log(0.01)
显然，- log(0.01) >> - log(0.5) , 可见，当结果的可能性变多时，A事件的信息量就变大了


---

作者：运筹之学
链接：https://www.zhihu.com/question/22178202/answer/49929786
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

那么，怎么衡量不确定性的变化的大小呢？怎么定义呢？这个问题不好回答，但是假设我们已经知道这个量已经存在了，不妨就叫做信息量，那么你觉得信息量起码该满足些什么特点呢？一，起码不是个负数吧，不然说句话还偷走信息呢~二，起码信息量和信息量之间可以相加吧！假如你告诉我的第一句话的信息量是3，在第一句话的基础上又告诉我一句话，额外信息量是4，那么两句话信息量加起来应该等于7吧！难道还能是5是9？三，刚刚已经提过，信息量跟概率有关系，但我们应该会觉得，信息量是连续依赖于概率的吧！就是说，某一个概率变化了0.0000001，那么这个信息量不应该变化很大。四，刚刚也提过，信息量大小跟可能结果数量有关。假如每一个可能的结果出现的概率一样，那么对于可能结果数量多的那个事件，新信息有更大的潜力具有更大的信息量，因为初始状态下不确定性更大。那有什么函数能满足上面四个条件呢？负的对数函数，也就是-log（x）！底数取大于1的数保证这个函数是非负的就行。前面再随便乘个正常数也行。a. 为什么不是正的？因为假如是正的，由于x是小于等于1的数，log（x）就小于等于0了。第一个特点满足。b. 咱们再来验证一下其他特点。三是最容易的。假如x是一个概率，那么log（x）是连续依赖于x的。donec。四呢？假如有n个可能结果，那么出现任意一个的概率是1/n，而-log(1/n)是n的增函数，没问题。d。最后验证二。由于-log(xy) = -log(x) -log(y)，所以也是对的。学数学的同学注意，这里的y可以是给定x的条件概率，当然也可以独立于x。By the way，这个函数是唯一的（除了还可以多乘上任意一个常数），有时间可以自己证明一下，或者查书。

---

从信息量的角度去切入，之前只从发生概率去看信息量的问题是不准确的，
对于是否，只有两个结果，我们需要增加一些结果才能推理出这个概念
比如现在我设计，我今天出门，有三种交通，单车、游艇、飞机

前面的解释比较愚钝而且错误的，应该首先去理解信息熵是不确定信息的程度，如果是遇到了比较确定且发生别的可能少的，信息量不多，信息熵很低。

应区分概率和熵的区别

---

信息熵 = 信息量乘以各自概率的正值