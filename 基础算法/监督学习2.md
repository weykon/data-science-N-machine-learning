基尼系数后，要熟悉的是？ 

## 决策树（Decision Tree）

1. 构建预处理的数据集，包括缺失数据的填充
    - 异常值处理：在数据中可能存在极端值或异常值，需要删除或替换这些数据。
    - 缺失值处理：在数据中可能存在缺失的值，需要使用一定的方法来填充这些缺失的值。常用的方法有：删除该样本、使用平均值或中位数填充、使用近邻数据填充等。
    - 数据标准化：在不同特征的数据可能存在较大的数值差距，需要使用一定的方法将数据转化为同等数值的范围。常用的方法有：归一化和标准化。
    
2. 计算信息熵，评估目标特征的不确定性。

3. 选择最佳分裂特征：通过选择增益最大的特征来作为当前的分裂特征。
    最佳分裂特征是从当前数据集中选择对数据划分带来最大信息增益的特征。信息增益是用来度量数据划分的好坏的指标，增益越大说明划分的效果越好。

    决策树通常使用ID3、C4.5等算法来决定最佳分裂特征。这些算法通常使用信息熵或基尼系数来计算信息增益。选择哪种方法，取决于实际应用的需求和目标。例如，在处理连续数值型数据时，可能需要使用C4.5算法，而在处理离散数值型数据时，则可以使用ID3算法。
  
    
4. 建立决策树：递归地基于当前的分裂特征，对数据集进行划分，直到满足停止条件为止。

5. 判定树的准确性：对训练出的决策树进行测试，评估其准确性。
  
### ID3: 
ID3（Iterative Dichotomiser 3）是一种基于息熵增益的决策树学习算法。算法的流程如下：
- 计算当前数据集的熵值。
- 计算每一个特征对熵值的影响，选择对熵值影响的特征作为当前节点的特征。
- 根据该特征的不同取值，将数据集分为多个子集。
- 对每个子数据集递归调用步骤1到步骤3，构建树。
- 如果所有样本都属于同一类别，则此节点即为点，算法结束；否则继续递归。
> 假设我们有一个训练数据集，其中每一行是一个本，每个样本有若干特征，以及对应的类别。我们的目是建立一个决策树，以此来预测新数据的类别。
> 1. 计算当前数据集的信息熵。信息熵是衡量数据集随机性的指标。
>  2. 对于每个特征，计算该特征对信息熵的贡献。
>  3. 选择最大贡献的特征作为当前分裂的最佳特征。
>  4. 对于最佳特征的每个取值，将当前数据集分割多个数据子集。
>  5. 对于每个子集，递归调用步骤1到步骤4，直到前子集的信息熵为0或者所有特征已经被使用。
>  6. 建立一个决策树，其中的每个节点对应一个征，每个叶节点对应一个类别。

### 如何对于每个特征，计算该特征对信息熵的贡献？ 
1. 计算原始数据的首要信息熵（ entropy ）；
2. 对每一个特征计算其对数据的信息增益；
3. 选择信息增益最大的特征作为最佳分裂特征；

信息增益计算公式：

Gain(S,A) = E(S) - ∑ [ E(T) * ( |T| / |S|) ]  
其中，E(S)表示原始数据的首要信息熵，E(T)表示某一特征下的信息熵，|T|和|S|分别表示T数据和S数据的数量。

通过计算每一个特征的信息增益，选择信息增益最大的特征作为最佳分裂特征，进而继续划分数据。

从本体信息熵，减去，子信息熵乘以（子数据量在总数据量的百分比）就得出一个特征的信息增益。


### 两种常见算法是干嘛的区分的
在处理连续数值型数据时，可能需要使用C4.5算法，而在处理离散数值型数据时，则可以使用ID3算法。

通俗来说，C4.5算法是在ID3算法的基础上进行改进，主要是为了解决ID3算法存在的一些问题，提高决策树的性能。

与ID3不同的是，C4.5使用信息增益比来选择特征，而不是纯粹的信息增益。信息增益比为一种对信息增益的标准化，以考虑特征不平衡的问题，即特征中可能存在大量值的情况。
 
> Weykon Kong  
意思是不是说ID3的增益后，进行一些参数的调节，对ID3算出来的最佳特征因为太多了，用C4.5进行容差处理？

> 是的，这个意思是正确的。ID3算法通过计算信息增益来确定最佳分裂特征，但是如果数据集中有太多的特征，那么最佳分裂特征的数量也就很多，C4.5就是对这种情况进行处理的。C4.5算法通过计算增益率来剪枝掉一些特征，从而避免过多的特征导致的过拟合。

### 剪枝操作
剪枝算法可以通过限制决策树的深度、移除不显著的分支，或在决策树生成后手动选择以提高模型的泛化性能。常见的剪枝算法有预剪枝和后剪枝两种。预剪枝通过在算法开始时设置一些限制来避免生成过深的决策树；后剪枝则在决策树生成后通过删除一些不必要的节点来缩短树的深度。

> Weykon Kong  
过拟合的情况，我设想一个情形，比如我的AI是需要识别人手写的0-9的数字，如果过拟合，出来的类别可能是不只10个数字的类别，比如对于1，有正反上下颠倒的类别1，是用C4.5就是想其能够只识别出1就足够的结果，是这样吗  

> 是的，算法通过剪枝处理避免了过拟合，即只识别出1就足够的结果。如果过拟合了，分类结果可能包括1的正反、上下、颠倒等多个类别，预测准确率不高。通过剪枝减小决策树的大小，降低了过拟合的风险  

- C4.5是J. Ross Quinlan于1993年发布的决策树算法的一个版本。该算法的名称是从它的版本号中获得的，即C（代表"C language"），4.5代表该算法是从版本4的ID3算法升级而来。因此，C4.5是ID3算法的升级版本。

---
### 随机森林
随机森林是一种基于决策树的集成学习方法，可以用于分类和回归。它通过多棵决策树组合起来，得到更好的预测性能。
### 支持向量机
支持向量机（SVM）是一种二分类算法，可以用于对数据进行分类，并在数据的两个类之间找到一个最佳的分界线。它的思想是通过构建一个超平面将数据划分为两类，并使得数据的间隔尽量大，同时使得分类误差最小。

学习完这两种算法，您可以继续深入了解机器学习的其他算法，如神经网络、k-近邻算法、朴素贝叶斯算法等。
